{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-small were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-small and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model.sentiment_model import Cls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Cls()\n",
    "state_dict = torch.load('/home/sangyeon/sentiment/sentiment_state_2.pt', map_location='cpu') \n",
    "# dataparallel로 학습했기에 module이 생겨서 새로운 dictionary생성\n",
    "new_state_dict = {}\n",
    "for k, v in state_dict.items():\n",
    "    name = k[7:]\n",
    "    new_state_dict[name] = v\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-small\")\n",
    "n_added_token=tokenizer.add_special_tokens({\"additional_special_tokens\":['[대분류]','[소분류]']})\n",
    "tokenizer.model_max_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n입력 문장\\n이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어. 너무 미안해.\\n[SEP]\\n내 능력이 부족한 거 같은데 그만 다녀야 될거같아.\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids , attention_mask =  tokenizer(\n",
    "    '이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어. 너무 미안해.'+'[SEP]'+'내 능력이 부족한 거 같은데 그만 다녀야 될거같아.'+' 감정 대분류 : [대분류] - 감정 소분류 : [소분류]',\n",
    "    padding='max_length',\n",
    "    return_tensors = 'pt',\n",
    "    return_token_type_ids=False,\n",
    "    add_special_tokens=False # [ClS]와 마지막 [SEP]은 사용하지 않을 것이기에 제외\n",
    "    ).values() \n",
    "idx1 = torch.argwhere(input_ids==32000)[0,1] # 대분류의 index\n",
    "idx2 = torch.argwhere(input_ids==32001)[0,1] # 소분류의 index\n",
    "\n",
    "'''\n",
    "입력 문장\n",
    "이번 프로젝트에서 발표를 하는데 내가 실수하는 바람에 우리 팀이 감점을 받았어. 너무 미안해.\n",
    "[SEP]\n",
    "내 능력이 부족한 거 같은데 그만 다녀야 될거같아.\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment1 = {\n",
    "    '분노': 0, \n",
    "    '기쁨': 1, \n",
    "    '불안': 2,\n",
    "    '당황': 3, \n",
    "    '슬픔': 4, \n",
    "    '상처': 5\n",
    "}\n",
    "sentiment2 = {\n",
    "    '노여워하는': 0,\n",
    "    '느긋': 1,\n",
    "    '걱정스러운': 2,\n",
    "    '당혹스러운': 3,\n",
    "    '당황': 4,\n",
    "    '마비된': 5,\n",
    "    '만족스러운': 6,\n",
    "    '배신당한': 7,\n",
    "    '버려진': 8,\n",
    "    '부끄러운': 9,\n",
    "    '분노': 10,\n",
    "    '불안': 11,\n",
    "    '비통한': 12,\n",
    "    '상처': 13,\n",
    "    '성가신': 14,\n",
    "    '스트레스 받는': 15,\n",
    "    '슬픔': 16,\n",
    "    '신뢰하는': 17,\n",
    "    '신이 난': 18,\n",
    "    '실망한': 19,\n",
    "    '악의적인': 20,\n",
    "    '안달하는': 21,\n",
    "    '안도': 22,\n",
    "    '억울한': 23,\n",
    "    '열등감': 24,\n",
    "    '염세적인': 25,\n",
    "    '외로운': 26,\n",
    "    '우울한': 27,\n",
    "    '고립된': 28,\n",
    "    '좌절한': 29,\n",
    "    '후회되는': 30,\n",
    "    '혐오스러운': 31,\n",
    "    '한심한': 32,\n",
    "    '자신하는': 33,\n",
    "    '기쁨': 34,\n",
    "    '툴툴대는': 35,\n",
    "    '남의 시선을 의식하는': 36,\n",
    "    '회의적인': 37,\n",
    "    '죄책감의': 38,\n",
    "    '혼란스러운': 39,\n",
    "    '초조한': 40,\n",
    "    '흥분': 41,\n",
    "    '충격 받은': 42,\n",
    "    '취약한': 43,\n",
    "    '편안한': 44,\n",
    "    '방어적인': 45,\n",
    "    '질투하는': 46,\n",
    "    '두려운': 47,\n",
    "    '눈물이 나는': 48,\n",
    "    '짜증내는': 49,\n",
    "    '조심스러운': 50,\n",
    "    '낙담한': 51,\n",
    "    '환멸을 느끼는': 52,\n",
    "    '희생된': 53,\n",
    "    '감사하는': 54,\n",
    "    '구역질 나는': 55,\n",
    "    '괴로워하는': 56,\n",
    "    '가난한, 불우한': 57\n",
    "}\n",
    "inverse_sentiment1 = {v:k for k,v in sentiment1.items()}\n",
    "inverse_sentiment2 = {v:k for k,v in sentiment2.items()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "out1 , out2 = model(input_ids , attention_mask, idx1, idx2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'불안'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_sentiment1[torch.argmax(out1).item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'두려운'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_sentiment2[torch.argmax(out2).item()]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4 | packaged by conda-forge | (main, Mar 30 2022, 08:38:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8f5dca216a5607e0bd8ff5c962a452461904293cf816e41fa3439b27ad78b520"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
